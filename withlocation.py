# -*- coding: utf-8 -*-
"""withlocation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T0QirBObO6kAO1YSBV7PHyYzfnUUB7-K
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import re
from sklearn import preprocessing
import warnings
warnings.filterwarnings('ignore')
import numpy as np

def  prune_text(df, text_field):
    df[text_field] = df[text_field].str.lower()
    df[text_field] = df[text_field].apply(lambda elem: re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", elem))  
    return df

data = pd.read_csv("data.csv")
location = pd.read_csv("louisiana_city.csv")
data.rename(columns={'New Classification':'label'},inplace=True)
data['cordinates'] = data['geo'].str[:-6]
required_columns = ['cordinates','tweet','label']
required_data = data[required_columns]
le = preprocessing.LabelEncoder()
required_data['label'] = le.fit_transform(required_data['label'])
required_data[['x_cor','y_cor']] = required_data.cordinates.str.split(",",expand=True)
required_data = prune_text(required_data,'tweet')
required_data.drop(columns=['cordinates'],inplace=True)
required_data['x_cor'] = required_data['x_cor'].str.strip()
required_data['y_cor'] = required_data['y_cor'].str.strip()
location['Latitude'] = location['Latitude'].astype(str).str.strip()
location['Longitude'] = location['Longitude'].astype(str).str.strip()
location_data = pd.merge(required_data, location,  how='left', left_on=['x_cor','y_cor'], right_on = ['Latitude','Longitude'])
final_data = location_data[['tweet','Location','label']]

# # Generate a pandas profiling report
# from pandas_profiling import ProfileReport
# # Trainset
# profile_train = ProfileReport(final_data, title="Pandas Profiling Report (Train)")
# profile_train.to_file(output_file="./NLP_COVID_EDA_train_Report.html")

# pip install pandas-profiling

# !pip3 install  contextualSpellCheck

import string
import re
import spacy
import contextualSpellCheck

class spe:
  def __init__(self):
    self.nlp = spacy.load('en_core_web_sm')
    self.nlp.add_pipe("contextual spellchecker", config={"max_edit_dist": 5})    
    self.punctuations = string.punctuation
    self.stopwords = spacy.lang.en.stop_words.STOP_WORDS

  def remove_urls(self,text):
      text = re.sub(r"\S*https?:\S*", "", text, flags=re.MULTILINE)
      return text

  def spacy_tokenizer(self,sentence):
      docs = self.nlp(sentence)

      # Lemmatize each token and convert each token into lowercase
      tokens = [word.lemma_.lower().strip() for word in docs]
      
      # Remove stopwords
      tokens = [word for word in tokens if word not in self.stopwords and word not in self.punctuations]
      
      # Remove links
      tokens = [self.remove_urls(word) for word in tokens]
      
      # return preprocessed list of tokens
      return tokens

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(final_data['tweet'],final_data['label'],test_size=0.2, random_state = 0)

from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer
from sklearn.naive_bayes import MultinomialNB

tokenizer = spe()

count_vect = CountVectorizer(tokenizer = tokenizer.spacy_tokenizer)
X_train_counts = count_vect.fit_transform(X_train)
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

clf = MultinomialNB().fit(X_train_tfidf, y_train)



# count_vect = CountVectorizer(tokenizer = tokenizer.spacy_tokenizer)
X_test_counts = count_vect.fit_transform(X_test)

ip = ['ValueError: Iterable over raw text documents expected, string object received.']

X_test_counts = count_vect.fit_transform(ip)
X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)

y_preds = clf.predict(X_test_tfidf)

X_test.iloc[0]

